{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\nfrom glob import glob\nimport time, gc\nimport cv2\n\nfrom tensorflow import keras\nimport matplotlib.image as mpimg\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.models import clone_model\nfrom keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nfrom tensorflow.python.client import device_lib\n\ndevice_lib.list_local_devices()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/train.csv')\ntest_df_ = pd.read_csv('/kaggle/input/bengaliai-cv19/test.csv')\nclass_map_df = pd.read_csv('/kaggle/input/bengaliai-cv19/class_map.csv')\nsample_sub_df = pd.read_csv('/kaggle/input/bengaliai-cv19/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_map_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Size of training data: {train_df_.shape}')\nprint(f'Size of test data: {test_df_.shape}')\nprint(f'Size of class map: {class_map_df.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods."},{"metadata":{"trusted":true},"cell_type":"code","source":"HEIGHT = 236\nWIDTH = 236\n\ndef get_n(df, field, n, top=True):\n    top_graphemes = df.groupby([field]).size().reset_index(name='counts')['counts'].sort_values(ascending=not top)[:n]\n    top_grapheme_roots = top_graphemes.index\n    top_grapheme_counts = top_graphemes.values\n    top_graphemes = class_map_df[class_map_df['component_type'] == field].reset_index().iloc[top_grapheme_roots]\n    top_graphemes.drop(['component_type', 'label'], axis=1, inplace=True)\n    top_graphemes.loc[:, 'count'] = top_grapheme_counts\n    return top_graphemes\n\ndef image_from_char(char):\n    image = Image.new('RGB', (WIDTH, HEIGHT))\n    draw = ImageDraw.Draw(image)\n    myfont = ImageFont.truetype('/kaggle/input/kalpurush-fonts/kalpurush-2.ttf', 120)\n    w, h = draw.textsize(char, font=myfont)\n    draw.text(((WIDTH - w) / 2,(HEIGHT - h) / 3), char, font=myfont)\n\n    return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of unique grapheme roots: {train_df_[\"grapheme_root\"].nunique()}')\nprint(f'Number of unique vowel diacritic: {train_df_[\"vowel_diacritic\"].nunique()}')\nprint(f'Number of unique consonant diacritic: {train_df_[\"consonant_diacritic\"].nunique()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Most used top 10 Grapheme Roots in training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_10_roots = get_n(train_df_, 'grapheme_root', 10)\ntop_10_roots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2, 5, figsize=(16, 8))\nax = ax.flatten()\n\nfor i in range(10):\n    ax[i].imshow(image_from_char(top_10_roots['component'].iloc[i]), cmap='Greys')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Least used 10 Grapheme Roots in training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"bottom_10_roots = get_n(train_df_, 'grapheme_root', 10, False)\nbottom_10_roots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(2, 5, figsize=(16, 8))\nax = ax.flatten()\n\nfor i in range(10):\n    ax[i].imshow(image_from_char(bottom_10_roots['component'].iloc[i]), cmap='Greys')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 5 Vowel Diacritic in taining data"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_vowels = get_n(train_df_, 'vowel_diacritic', 5)\ntop_5_vowels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 5, figsize=(16, 8))\nax = ax.flatten()\n\nfor i in range(5):\n    ax[i].imshow(image_from_char(top_5_vowels['component'].iloc[i]), cmap='Greys')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top 5 Consonant Diacritic in training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_consonants = get_n(train_df_, 'consonant_diacritic', 5)\ntop_5_consonants","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 5, figsize=(16, 8))\nax = ax.flatten()\n\nfor i in range(5):\n    ax[i].imshow(image_from_char(top_5_consonants['component'].iloc[i]), cmap='Greys')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_ = train_df_.drop(['grapheme'], axis=1, inplace=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_df_[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE=64\nN_CHANNELS=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's apply some image processing (credits: [this kernel](https://www.kaggle.com/shawon10/bangla-graphemes-image-processing-deep-cnn)) while resizing the images, which will center crop the region of interest from the original images."},{"metadata":{"trusted":true},"cell_type":"code","source":"def resize(df, size=64, need_progress_bar=True):\n    resized = {}\n    resize_size=64\n    if need_progress_bar:\n        for i in tqdm(range(df.shape[0])):\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    else:\n        for i in range(df.shape[0]):\n            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    resized = pd.DataFrame(resized).T\n    return resized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dummies(df):\n    cols = []\n    for col in df:\n        cols.append(pd.get_dummies(df[col].astype(str)))\n    return pd.concat(cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import GlobalAveragePooling2D, Activation, Add\nfrom keras.regularizers import l2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WEIGHT_DECAY = 5e-4\n\ndef resnext_block(x, output_width, cardinality, width):\n    x = Conv2D(filters=output_width, padding='same', kernel_size=3)(x)\n    inp = x\n    subblocks = []\n    \n    for i in range(cardinality):\n        y = Conv2D(filters=width, padding='same', kernel_size=1)(x)\n        y = Activation('relu')(y)\n        y = Conv2D(filters=width, padding='same', kernel_size=3)(y)\n        y = Activation('relu')(y)\n        y = Conv2D(filters=output_width, padding='same', kernel_size=1)(y)\n        y = Activation('relu')(y)\n        subblocks.append(y)\n    \n    x = Add()(subblocks)\n    x = Add()([x, inp])\n    \n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    \n    return x\n\ndef init_block(x):\n    x = Conv2D(32, (5, 5), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(WEIGHT_DECAY))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPool2D(pool_size=(2, 2))(x)\n\n    return x\n\ndef resnext(blocks, input_size=(IMG_SIZE, IMG_SIZE, 1)):\n    x = Input(shape=input_size)\n    inp = x\n    \n    x = init_block(x)\n    \n    for b in blocks:\n        for i in range(b[2]):\n            x = resnext_block(x, b[0], b[3], b[1])\n        x = MaxPool2D()(x)\n    \n    x = GlobalAveragePooling2D()(x)\n    \n    x = Dense(1024, activation = \"relu\")(x)\n    x = Dropout(rate=0.3)(x)\n    x = Dense(512, activation = \"relu\")(x)\n    \n    head_root = Dense(168, activation = 'softmax', name='dense_1')(x)\n    head_vowel = Dense(11, activation = 'softmax', name='dense_2')(x)\n    head_consonant = Dense(7, activation = 'softmax', name='dense_3')(x)\n\n    model = Model(inputs=inp, outputs=[head_root, head_vowel, head_consonant])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = resnext([\n    (64, 32, 2, 16), \n    (128, 64, 2, 16), \n    (256, 128, 2, 16), \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''ResNeXt models for Keras.\n# Reference\n- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf))\n'''\nfrom __future__ import print_function\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.core import Activation\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.pooling import GlobalAveragePooling2D, GlobalMaxPooling2D, MaxPooling2D\nfrom keras.layers import Input\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras_applications.imagenet_utils import _obtain_input_shape\nimport keras.backend as K\n\nCIFAR_TH_WEIGHTS_PATH = ''\nCIFAR_TF_WEIGHTS_PATH = ''\nCIFAR_TH_WEIGHTS_PATH_NO_TOP = ''\nCIFAR_TF_WEIGHTS_PATH_NO_TOP = ''\n\nIMAGENET_TH_WEIGHTS_PATH = ''\nIMAGENET_TF_WEIGHTS_PATH = ''\nIMAGENET_TH_WEIGHTS_PATH_NO_TOP = ''\nIMAGENET_TF_WEIGHTS_PATH_NO_TOP = ''\n\n\ndef ResNext(input_shape=None, depth=29, cardinality=8, width=64, weight_decay=5e-4,\n            include_top=True, weights=None, input_tensor=None,\n            pooling=None, classes=10):\n    \"\"\"Instantiate the ResNeXt architecture. Note that ,\n        when using TensorFlow for best performance you should set\n        `image_data_format=\"channels_last\"` in your Keras config\n        at ~/.keras/keras.json.\n        The model are compatible with both\n        TensorFlow and Theano. The dimension ordering\n        convention used by the model is the one\n        specified in your Keras config file.\n        # Arguments\n            depth: number or layers in the ResNeXt model. Can be an\n                integer or a list of integers.\n            cardinality: the size of the set of transformations\n            width: multiplier to the ResNeXt width (number of filters)\n            weight_decay: weight decay (l2 norm)\n            include_top: whether to include the fully-connected\n                layer at the top of the network.\n            weights: `None` (random initialization)\n            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n                to use as image input for the model.\n            input_shape: optional shape tuple, only to be specified\n                if `include_top` is False (otherwise the input shape\n                has to be `(32, 32, 3)` (with `tf` dim ordering)\n                or `(3, 32, 32)` (with `th` dim ordering).\n                It should have exactly 3 inputs channels,\n                and width and height should be no smaller than 8.\n                E.g. `(200, 200, 3)` would be one valid value.\n            pooling: Optional pooling mode for feature extraction\n                when `include_top` is `False`.\n                - `None` means that the output of the model will be\n                    the 4D tensor output of the\n                    last convolutional layer.\n                - `avg` means that global average pooling\n                    will be applied to the output of the\n                    last convolutional layer, and thus\n                    the output of the model will be a 2D tensor.\n                - `max` means that global max pooling will\n                    be applied.\n            classes: optional number of classes to classify images\n                into, only to be specified if `include_top` is True, and\n                if no `weights` argument is specified.\n        # Returns\n            A Keras model instance.\n        \"\"\"\n\n    if weights not in {'cifar10', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `cifar10` '\n                         '(pre-training on CIFAR-10).')\n\n    if weights == 'cifar10' and include_top and classes != 10:\n        raise ValueError('If using `weights` as CIFAR 10 with `include_top`'\n                         ' as true, `classes` should be 10')\n\n    if type(depth) == int:\n        if (depth - 2) % 9 != 0:\n            raise ValueError('Depth of the network must be such that (depth - 2)'\n                             'should be divisible by 9.')\n\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=32,\n                                      min_size=8,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    o1, o2, o3 = __create_res_next(classes, img_input, include_top, depth, cardinality, width,\n                          weight_decay, pooling)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs=inputs, outputs=[o1, o2, o3], name='resnext')\n\n    # load weights\n    if weights == 'cifar10':\n        if (depth == 29) and (cardinality == 8) and (width == 64):\n            # Default parameters match. Weights for this model exist:\n\n            if K.image_data_format() == 'channels_first':\n                if include_top:\n                    weights_path = get_file('resnext_cifar_10_8_64_th_dim_ordering_th_kernels.h5',\n                                            CIFAR_TH_WEIGHTS_PATH,\n                                            cache_subdir='models')\n                else:\n                    weights_path = get_file('resnext_cifar_10_8_64_th_dim_ordering_th_kernels_no_top.h5',\n                                            CIFAR_TH_WEIGHTS_PATH_NO_TOP,\n                                            cache_subdir='models')\n\n                model.load_weights(weights_path)\n\n                if K.backend() == 'tensorflow':\n                    warnings.warn('You are using the TensorFlow backend, yet you '\n                                  'are using the Theano '\n                                  'image dimension ordering convention '\n                                  '(`image_dim_ordering=\"th\"`). '\n                                  'For best performance, set '\n                                  '`image_dim_ordering=\"tf\"` in '\n                                  'your Keras config '\n                                  'at ~/.keras/keras.json.')\n                    convert_all_kernels_in_model(model)\n            else:\n                if include_top:\n                    weights_path = get_file('resnext_cifar_10_8_64_tf_dim_ordering_tf_kernels.h5',\n                                            CIFAR_TF_WEIGHTS_PATH,\n                                            cache_subdir='models')\n                else:\n                    weights_path = get_file('resnext_cifar_10_8_64_tf_dim_ordering_tf_kernels_no_top.h5',\n                                            CIFAR_TF_WEIGHTS_PATH_NO_TOP,\n                                            cache_subdir='models')\n\n                model.load_weights(weights_path)\n\n                if K.backend() == 'theano':\n                    convert_all_kernels_in_model(model)\n\n    return model\n\n\ndef ResNextImageNet(input_shape=None, depth=[3, 4, 6, 3], cardinality=32, width=4, weight_decay=5e-4,\n                    include_top=True, weights=None, input_tensor=None,\n                    pooling=None, classes=1000):\n    \"\"\" Instantiate the ResNeXt architecture for the ImageNet dataset. Note that ,\n        when using TensorFlow for best performance you should set\n        `image_data_format=\"channels_last\"` in your Keras config\n        at ~/.keras/keras.json.\n        The model are compatible with both\n        TensorFlow and Theano. The dimension ordering\n        convention used by the model is the one\n        specified in your Keras config file.\n        # Arguments\n            depth: number or layers in the each block, defined as a list.\n                ResNeXt-50 can be defined as [3, 4, 6, 3].\n                ResNeXt-101 can be defined as [3, 4, 23, 3].\n                Defaults is ResNeXt-50.\n            cardinality: the size of the set of transformations\n            width: multiplier to the ResNeXt width (number of filters)\n            weight_decay: weight decay (l2 norm)\n            include_top: whether to include the fully-connected\n                layer at the top of the network.\n            weights: `None` (random initialization) or `imagenet` (trained\n                on ImageNet)\n            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n                to use as image input for the model.\n            input_shape: optional shape tuple, only to be specified\n                if `include_top` is False (otherwise the input shape\n                has to be `(224, 224, 3)` (with `tf` dim ordering)\n                or `(3, 224, 224)` (with `th` dim ordering).\n                It should have exactly 3 inputs channels,\n                and width and height should be no smaller than 8.\n                E.g. `(200, 200, 3)` would be one valid value.\n            pooling: Optional pooling mode for feature extraction\n                when `include_top` is `False`.\n                - `None` means that the output of the model will be\n                    the 4D tensor output of the\n                    last convolutional layer.\n                - `avg` means that global average pooling\n                    will be applied to the output of the\n                    last convolutional layer, and thus\n                    the output of the model will be a 2D tensor.\n                - `max` means that global max pooling will\n                    be applied.\n            classes: optional number of classes to classify images\n                into, only to be specified if `include_top` is True, and\n                if no `weights` argument is specified.\n        # Returns\n            A Keras model instance.\n        \"\"\"\n\n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `imagenet` '\n                         '(pre-training on ImageNet).')\n\n    if weights == 'imagenet' and include_top and classes != 1000:\n        raise ValueError('If using `weights` as imagenet with `include_top`'\n                         ' as true, `classes` should be 1000')\n\n    if type(depth) == int and (depth - 2) % 9 != 0:\n        raise ValueError('Depth of the network must be such that (depth - 2)'\n                         'should be divisible by 9.')\n    # Determine proper input shape\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=112,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    x = __create_res_next_imagenet(classes, img_input, include_top, depth, cardinality, width,\n                                   weight_decay, pooling)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name='resnext')\n\n    # load weights\n    if weights == 'imagenet':\n        if (depth == [3, 4, 6, 3]) and (cardinality == 32) and (width == 4):\n            # Default parameters match. Weights for this model exist:\n\n            if K.image_data_format() == 'channels_first':\n                if include_top:\n                    weights_path = get_file('resnext_imagenet_32_4_th_dim_ordering_th_kernels.h5',\n                                            IMAGENET_TH_WEIGHTS_PATH,\n                                            cache_subdir='models')\n                else:\n                    weights_path = get_file('resnext_imagenet_32_4_th_dim_ordering_th_kernels_no_top.h5',\n                                            IMAGENET_TH_WEIGHTS_PATH_NO_TOP,\n                                            cache_subdir='models')\n\n                model.load_weights(weights_path)\n\n                if K.backend() == 'tensorflow':\n                    warnings.warn('You are using the TensorFlow backend, yet you '\n                                  'are using the Theano '\n                                  'image dimension ordering convention '\n                                  '(`image_dim_ordering=\"th\"`). '\n                                  'For best performance, set '\n                                  '`image_dim_ordering=\"tf\"` in '\n                                  'your Keras config '\n                                  'at ~/.keras/keras.json.')\n                    convert_all_kernels_in_model(model)\n            else:\n                if include_top:\n                    weights_path = get_file('resnext_imagenet_32_4_tf_dim_ordering_tf_kernels.h5',\n                                            IMAGENET_TF_WEIGHTS_PATH,\n                                            cache_subdir='models')\n                else:\n                    weights_path = get_file('resnext_imagenet_32_4_tf_dim_ordering_tf_kernels_no_top.h5',\n                                            IMAGENET_TF_WEIGHTS_PATH_NO_TOP,\n                                            cache_subdir='models')\n\n                model.load_weights(weights_path)\n\n                if K.backend() == 'theano':\n                    convert_all_kernels_in_model(model)\n\n    return model\n\n\ndef __initial_conv_block(input, weight_decay=5e-4):\n    ''' Adds an initial convolution block, with batch normalization and relu activation\n    Args:\n        input: input tensor\n        weight_decay: weight decay factor\n    Returns: a keras tensor\n    '''\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    x = Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(weight_decay))(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation('relu')(x)\n\n    return x\n\n\ndef __initial_conv_block_imagenet(input, weight_decay=5e-4):\n    ''' Adds an initial conv block, with batch norm and relu for the inception resnext\n    Args:\n        input: input tensor\n        weight_decay: weight decay factor\n    Returns: a keras tensor\n    '''\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    x = Conv2D(64, (7, 7), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(weight_decay), strides=(2, 2))(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation('relu')(x)\n\n    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n\n    return x\n\n\ndef __grouped_convolution_block(input, grouped_channels, cardinality, strides, weight_decay=5e-4):\n    ''' Adds a grouped convolution block. It is an equivalent block from the paper\n    Args:\n        input: input tensor\n        grouped_channels: grouped number of filters\n        cardinality: cardinality factor describing the number of groups\n        strides: performs strided convolution for downscaling if > 1\n        weight_decay: weight decay term\n    Returns: a keras tensor\n    '''\n    init = input\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    group_list = []\n\n    if cardinality == 1:\n        # with cardinality 1, it is a standard convolution\n        x = Conv2D(grouped_channels, (3, 3), padding='same', use_bias=False, strides=(strides, strides),\n                   kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(init)\n        x = BatchNormalization(axis=channel_axis)(x)\n        x = Activation('relu')(x)\n        return x\n\n    for c in range(cardinality):\n        x = Lambda(lambda z: z[:, :, :, c * grouped_channels:(c + 1) * grouped_channels]\n        if K.image_data_format() == 'channels_last' else\n        lambda z: z[:, c * grouped_channels:(c + 1) * grouped_channels, :, :])(input)\n\n        x = Conv2D(grouped_channels, (3, 3), padding='same', use_bias=False, strides=(strides, strides),\n                   kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n\n        group_list.append(x)\n\n    group_merge = concatenate(group_list, axis=channel_axis)\n    x = BatchNormalization(axis=channel_axis)(group_merge)\n    x = Activation('relu')(x)\n\n    return x\n\n\ndef __bottleneck_block(input, filters=64, cardinality=8, strides=1, weight_decay=5e-4):\n    ''' Adds a bottleneck block\n    Args:\n        input: input tensor\n        filters: number of output filters\n        cardinality: cardinality factor described number of\n            grouped convolutions\n        strides: performs strided convolution for downsampling if > 1\n        weight_decay: weight decay factor\n    Returns: a keras tensor\n    '''\n    init = input\n\n    grouped_channels = int(filters / cardinality)\n    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n\n    # Check if input number of filters is same as 16 * k, else create convolution2d for this input\n    if K.image_data_format() == 'channels_first':\n        if init._keras_shape[1] != 2 * filters:\n            init = Conv2D(filters * 2, (1, 1), padding='same', strides=(strides, strides),\n                          use_bias=False, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(init)\n            init = BatchNormalization(axis=channel_axis)(init)\n    else:\n        if init._keras_shape[-1] != 2 * filters:\n            init = Conv2D(filters * 2, (1, 1), padding='same', strides=(strides, strides),\n                          use_bias=False, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(init)\n            init = BatchNormalization(axis=channel_axis)(init)\n\n    x = Conv2D(filters, (1, 1), padding='same', use_bias=False,\n               kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation('relu')(x)\n\n    x = __grouped_convolution_block(x, grouped_channels, cardinality, strides, weight_decay)\n\n    x = Conv2D(filters * 2, (1, 1), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(weight_decay))(x)\n    x = BatchNormalization(axis=channel_axis)(x)\n\n    x = add([init, x])\n    x = Activation('relu')(x)\n\n    return x\n\n\ndef __create_res_next(nb_classes, img_input, include_top, depth=29, cardinality=8, width=4,\n                      weight_decay=5e-4, pooling=None):\n    ''' Creates a ResNeXt model with specified parameters\n    Args:\n        nb_classes: Number of output classes\n        img_input: Input tensor or layer\n        include_top: Flag to include the last dense layer\n        depth: Depth of the network. Can be an positive integer or a list\n               Compute N = (n - 2) / 9.\n               For a depth of 56, n = 56, N = (56 - 2) / 9 = 6\n               For a depth of 101, n = 101, N = (101 - 2) / 9 = 11\n        cardinality: the size of the set of transformations.\n               Increasing cardinality improves classification accuracy,\n        width: Width of the network.\n        weight_decay: weight_decay (l2 norm)\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n    Returns: a Keras Model\n    '''\n\n    if type(depth) is list or type(depth) is tuple:\n        # If a list is provided, defer to user how many blocks are present\n        N = list(depth)\n    else:\n        # Otherwise, default to 3 blocks each of default number of group convolution blocks\n        N = [(depth - 2) // 9 for _ in range(3)]\n\n    filters = cardinality * width\n    filters_list = []\n\n    for i in range(len(N)):\n        filters_list.append(filters)\n        filters *= 2  # double the size of the filters\n\n    x = __initial_conv_block(img_input, weight_decay)\n\n    # block 1 (no pooling)\n    for i in range(N[0]):\n        x = __bottleneck_block(x, filters_list[0], cardinality, strides=1, weight_decay=weight_decay)\n\n    N = N[1:]  # remove the first block from block definition list\n    filters_list = filters_list[1:]  # remove the first filter from the filter list\n\n    # block 2 to N\n    for block_idx, n_i in enumerate(N):\n        for i in range(n_i):\n            if i == 0:\n                x = __bottleneck_block(x, filters_list[block_idx], cardinality, strides=2,\n                                       weight_decay=weight_decay)\n            else:\n                x = __bottleneck_block(x, filters_list[block_idx], cardinality, strides=1,\n                                       weight_decay=weight_decay)\n\n    if include_top:\n        x = GlobalAveragePooling2D()(x)\n        o1 = Dense(168, use_bias=False, kernel_regularizer=l2(weight_decay),\n                  kernel_initializer='he_normal', activation='softmax', name='dense_1')(x)\n        o2 = Dense(11, use_bias=False, kernel_regularizer=l2(weight_decay),\n                  kernel_initializer='he_normal', activation='softmax', name='dense_2')(x)\n        o3 = Dense(7, use_bias=False, kernel_regularizer=l2(weight_decay),\n                  kernel_initializer='he_normal', activation='softmax', name='dense_3')(x)\n    else:\n        if pooling == 'avg':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == 'max':\n            x = GlobalMaxPooling2D()(x)\n\n    return o1, o2, o3\n\n\ndef __create_res_next_imagenet(nb_classes, img_input, include_top, depth, cardinality=32, width=4,\n                               weight_decay=5e-4, pooling=None):\n    ''' Creates a ResNeXt model with specified parameters\n    Args:\n        nb_classes: Number of output classes\n        img_input: Input tensor or layer\n        include_top: Flag to include the last dense layer\n        depth: Depth of the network. List of integers.\n               Increasing cardinality improves classification accuracy,\n        width: Width of the network.\n        weight_decay: weight_decay (l2 norm)\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n    Returns: a Keras Model\n    '''\n\n    if type(depth) is list or type(depth) is tuple:\n        # If a list is provided, defer to user how many blocks are present\n        N = list(depth)\n    else:\n        # Otherwise, default to 3 blocks each of default number of group convolution blocks\n        N = [(depth - 2) // 9 for _ in range(3)]\n\n    filters = cardinality * width\n    filters_list = []\n\n    for i in range(len(N)):\n        filters_list.append(filters)\n        filters *= 2  # double the size of the filters\n\n    x = __initial_conv_block_imagenet(img_input, weight_decay)\n\n    # block 1 (no pooling)\n    for i in range(N[0]):\n        x = __bottleneck_block(x, filters_list[0], cardinality, strides=1, weight_decay=weight_decay)\n\n    N = N[1:]  # remove the first block from block definition list\n    filters_list = filters_list[1:]  # remove the first filter from the filter list\n\n    # block 2 to N\n    for block_idx, n_i in enumerate(N):\n        for i in range(n_i):\n            if i == 0:\n                x = __bottleneck_block(x, filters_list[block_idx], cardinality, strides=2,\n                                       weight_decay=weight_decay)\n            else:\n                x = __bottleneck_block(x, filters_list[block_idx], cardinality, strides=1,\n                                       weight_decay=weight_decay)\n\n    if include_top:\n        x = GlobalAveragePooling2D()(x)\n        x = Dense(nb_classes, use_bias=False, kernel_regularizer=l2(weight_decay),\n                  kernel_initializer='he_normal', activation='softmax')(x)\n    else:\n        if pooling == 'avg':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == 'max':\n            x = GlobalMaxPooling2D()(x)\n\n    return x\n\nmodel = ResNext((IMG_SIZE, IMG_SIZE, 1), depth=20, cardinality=8, width=32)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Basic Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = Input(shape = (IMG_SIZE, IMG_SIZE, 1))\n\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1))(inputs)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Flatten()(model)\nmodel = Dense(1024, activation = \"relu\")(model)\nmodel = Dropout(rate=0.3)(model)\ndense = Dense(512, activation = \"relu\")(model)\n\nhead_root = Dense(168, activation = 'softmax')(dense)\nhead_vowel = Dense(11, activation = 'softmax')(dense)\nhead_consonant = Dense(7, activation = 'softmax')(dense)\n\nmodel = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.regularizers import l2\n\nWEIGHT_DECAY = 5e-4\n\ndef init_block(x):\n    x = Conv2D(64, (3, 3), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(WEIGHT_DECAY))(x)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation('relu')(x)\n    return x\n\ndef grouped_conv_block(x):\n    \n    return x\n\ndef bottleneck_block(x):\n    inp = x\n    x = Conv2D(filters, (1, 1), padding='same', use_bias=False,\n               kernel_initializer='he_normal', kernel_regularizer=l2(WEIGHT_DECAY))(input)\n    x = BatchNormalization(axis=channel_axis)(x)\n    x = Activation('relu')(x)\n    \n    x = grouped_conv_block(x, grouped_channels, cardinality, strides, weight_decay)\n\n    x = Conv2D(filters * 2, (1, 1), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(weight_decay))(x)\n    x = BatchNormalization(axis=channel_axis)(x)\n\n    x = add([inp, x])\n    x = Activation('relu')(x)\n    \n    return x\n\ndef resnext(size=(IMG_SIZE, IMG_SIZE)):\n    x = Input(size)\n    \n    x = init_block(x)\n    \n    for i in range()\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize the 3-tailed (3 output) CNN by plotting it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import recall_score\nimport keras as K\n# m1 = K.metrics.Recall()\n# m2 = K.metrics.Recall()\n# m3 = K.metrics.Recall()\n# def mean_pred(y_true, y_pred):\n#     m1.update_state(y_true[:, 0], y_pred[:, 0])\n#     m2.update_state(y_true[:, 1], y_pred[:, 1])\n#     m3.update_state(y_true[:, 2], y_pred[:, 2])\n#     return m1.result()*2 + m2.result() + m3.result()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', K.metrics.Recall()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set a learning rate annealer. Learning rate will be half after 3 epochs if accuracy is not increased\nlearning_rate_reduction_root = ReduceLROnPlateau(monitor='dense_1_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_2_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)\nlearning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_3_accuracy', \n                                            patience=3, \n                                            verbose=1,\n                                            factor=0.5, \n                                            min_lr=0.00001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 75\nepochs = 40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n\n    def flow(self,\n             x,\n             y=None,\n             batch_size=32,\n             shuffle=True,\n             sample_weight=None,\n             seed=None,\n             save_to_dir=None,\n             save_prefix='',\n             save_format='png',\n             subset=None):\n\n        targets = None\n        target_lengths = {}\n        ordered_outputs = []\n        for output, target in y.items():\n            if targets is None:\n                targets = target\n            else:\n                targets = np.concatenate((targets, target), axis=1)\n            target_lengths[output] = target.shape[1]\n            ordered_outputs.append(output)\n\n\n        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n                                         shuffle=shuffle):\n            target_dict = {}\n            i = 0\n            for output in ordered_outputs:\n                target_length = target_lengths[output]\n                target_dict[output] = flowy[:, i: i + target_length]\n                i += target_length\n\n            yield flowx, target_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HEIGHT = 137\nWIDTH = 236","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"histories = []\nfor _ in range(2):\n    for i in range(4):\n        train_df = pd.merge(pd.read_parquet(f'/kaggle/input/bengaliai-cv19/train_image_data_{i}.parquet'), train_df_, on='image_id').drop(['image_id'], axis=1)\n\n        # Visualize few samples of current training dataset\n        fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n        count=0\n        for row in ax:\n            for col in row:\n                col.imshow(resize(train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1).iloc[[count]], need_progress_bar=False).values.reshape(-1).reshape(IMG_SIZE, IMG_SIZE).astype(np.float64))\n                count += 1\n        plt.show()\n\n        X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n        X_train = resize(X_train)/255\n\n        # CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n        X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n\n        Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n        Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n        Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n\n        print(f'Training images: {X_train.shape}')\n        print(f'Training labels root: {Y_train_root.shape}')\n        print(f'Training labels vowel: {Y_train_vowel.shape}')\n        print(f'Training labels consonants: {Y_train_consonant.shape}')\n\n        # Divide the data into training and validation set\n        x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=666)\n        del train_df\n        del X_train\n        del Y_train_root, Y_train_vowel, Y_train_consonant\n\n        # Data augmentation for creating more training data\n        datagen = MultiOutputDataGenerator(\n            featurewise_center=False,  # set input mean to 0 over the dataset\n            samplewise_center=False,  # set each sample mean to 0\n            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n            samplewise_std_normalization=False,  # divide each input by its std\n            zca_whitening=False,  # apply ZCA whitening\n            rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n            zoom_range = 0.15, # Randomly zoom image \n            width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n            height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n            horizontal_flip=False,  # randomly flip images\n            vertical_flip=False)  # randomly flip images\n\n\n        # This will just calculate parameters required to augment the given data. This won't perform any augmentations\n        datagen.fit(x_train)\n\n        # Fit the model\n        history = model.fit_generator(datagen.flow(x_train, {'dense_1': y_train_root, 'dense_2': y_train_vowel, 'dense_3': y_train_consonant}, batch_size=batch_size),\n                                  epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n                                  steps_per_epoch=x_train.shape[0] // batch_size, \n                                  callbacks=[learning_rate_reduction_root, learning_rate_reduction_vowel, learning_rate_reduction_consonant])\n\n        histories.append(history)\n\n        # Delete to reduce memory usage\n        del x_train\n        del x_test\n        del y_train_root\n        del y_test_root\n        del y_train_vowel\n        del y_test_vowel\n        del y_train_consonant\n        del y_test_consonant\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\ndef plot_loss(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['loss'], label='train_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_3_loss'], label='train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_4_loss'], label='train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['dense_5_loss'], label='train_consonant_loss')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_dense_3_loss'], label='val_train_root_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_4_loss'], label='val_train_vowel_loss')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_5_loss'], label='val_train_consonant_loss')\n    \n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Loss')\n    plt.legend(loc='upper right')\n    plt.show()\n\ndef plot_acc(his, epoch, title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0, epoch), his.history['dense_3_accuracy'], label='train_root_acc')\n    plt.plot(np.arange(0, epoch), his.history['dense_4_accuracy'], label='train_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['dense_5_accuracy'], label='train_consonant_accuracy')\n    \n    plt.plot(np.arange(0, epoch), his.history['val_dense_3_accuracy'], label='val_root_acc')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_4_accuracy'], label='val_vowel_accuracy')\n    plt.plot(np.arange(0, epoch), his.history['val_dense_5_accuracy'], label='val_consonant_accuracy')\n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='upper right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in range(4):\n    plot_loss(histories[dataset], epochs, f'Training Dataset: {dataset}')\n    plot_acc(histories[dataset], epochs, f'Training Dataset: {dataset}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del histories\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_dict = {\n    'grapheme_root': [],\n    'vowel_diacritic': [],\n    'consonant_diacritic': []\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\ntarget=[] # model predictions placeholder\nrow_id=[] # row_id place holder\nfor i in range(4):\n    df_test_img = pd.read_parquet('/kaggle/input/bengaliai-cv19/test_image_data_{}.parquet'.format(i)) \n    df_test_img.set_index('image_id', inplace=True)\n\n    X_test = resize(df_test_img, need_progress_bar=False)/255\n    X_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n    \n    preds = model.predict(X_test)\n\n    for i, p in enumerate(preds_dict):\n        preds_dict[p] = np.argmax(preds[i], axis=1)\n\n    for k,id in enumerate(df_test_img.index.values):  \n        for i,comp in enumerate(components):\n            id_sample=id+'_'+comp\n            row_id.append(id_sample)\n            target.append(preds_dict[comp][k])\n    del df_test_img\n    del X_test\n    gc.collect()\n\ndf_sample = pd.DataFrame(\n    {\n        'row_id': row_id,\n        'target':target\n    },\n    columns = ['row_id','target'] \n)\ndf_sample.to_csv('submission.csv',index=False)\ndf_sample.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"108585220d384f1d87f3c8444587ae5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_e4855ad98fad40deb899d57d401ae8c3","max":50210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0b119ea39cc420ea11296b456d078d8","value":50210}},"12ab2aed987f4a2e9bed87f8df6fbace":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2e73348f1084afe92c3f03035d8606b","IPY_MODEL_b57249febd6c457baa5d4ba55f379ec1"],"layout":"IPY_MODEL_c9970f6bf5c54ff8808f97fdeb286846"}},"3540aa1072634a95b56adbc63b871c44":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"570a0b20e27c418687a1a1b252721597":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62710fc6ed3346d5894184aa95d3a0dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75b62e2e75604016b1b25b7dfafd055e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"7dc300f0a85241fe8385df414e8a5c97":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"819ad327d8de4df6ab1c59de568544d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1f711e1aaf24dc197f468a9a1d2dac2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2e45ab503b34cd68d5ffd0f81cb33f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2e73348f1084afe92c3f03035d8606b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_3540aa1072634a95b56adbc63b871c44","max":50210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfd07d8d18ed471fb9ea717da22823f0","value":50210}},"b57249febd6c457baa5d4ba55f379ec1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc99860fe34c4945b98db54863554527","placeholder":"","style":"IPY_MODEL_570a0b20e27c418687a1a1b252721597","value":" 50210/50210 [00:34&lt;00:00, 1472.92it/s]"}},"c40188b4079d4a32a3c40d43714ffdeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_62710fc6ed3346d5894184aa95d3a0dd","max":50210,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75b62e2e75604016b1b25b7dfafd055e","value":50210}},"c9970f6bf5c54ff8808f97fdeb286846":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd962aa4c7514f50a817344fcc4281c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7dc300f0a85241fe8385df414e8a5c97","placeholder":"","style":"IPY_MODEL_b2e45ab503b34cd68d5ffd0f81cb33f0","value":" 50210/50210 [00:34&lt;00:00, 1453.38it/s]"}},"cfd07d8d18ed471fb9ea717da22823f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"dc99860fe34c4945b98db54863554527":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddd90cb7822f4ceca0f5bbdb623d9153":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_108585220d384f1d87f3c8444587ae5b","IPY_MODEL_cd962aa4c7514f50a817344fcc4281c0"],"layout":"IPY_MODEL_e175ccd71bbc41ff8aa02b43f00de935"}},"e175ccd71bbc41ff8aa02b43f00de935":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4855ad98fad40deb899d57d401ae8c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0b119ea39cc420ea11296b456d078d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"f8a313a8731e4aee9ba940aa7bc795b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c40188b4079d4a32a3c40d43714ffdeb","IPY_MODEL_ffa184123d1f45c5a7296f99465f2a08"],"layout":"IPY_MODEL_fcd628bac3174930bdb3727134928c69"}},"fcd628bac3174930bdb3727134928c69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffa184123d1f45c5a7296f99465f2a08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_819ad327d8de4df6ab1c59de568544d6","placeholder":"","style":"IPY_MODEL_b1f711e1aaf24dc197f468a9a1d2dac2","value":" 50210/50210 [00:34&lt;00:00, 1436.32it/s]"}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}