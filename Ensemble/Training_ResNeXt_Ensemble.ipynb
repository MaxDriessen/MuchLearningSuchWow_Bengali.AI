{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali.AI Competition - ResNeXt Training (Ensemble)\n",
    "\n",
    "### Team MuchLearningSuchWow\n",
    "\n",
    "This notebook contains code for training the ResNeXt network we used in our ensemble. It is connected to Weights and Biases in order to keep track of progress and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import psutil\n",
    "\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, Add, MaxPool2D, Dense, \\\n",
    "    Dropout, GlobalAveragePooling2D, Concatenate, Input, Flatten, AveragePooling2D, Add\n",
    "from keras import Model\n",
    "from keras.regularizers import l2\n",
    "    \n",
    "import gc\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'input/bengaliai-cv19/train.csv'\n",
    "model_filename = 'output/model_resnext.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('output'):\n",
    "    os.mkdir('output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = pd.read_csv(train_filename)\n",
    "train_df_ = train_df_.drop(['grapheme'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project='bengali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = run.config\n",
    "config.blocks = [\n",
    "    {\n",
    "        'width': 64,\n",
    "        'output_width': 128,\n",
    "        'cardinality': 24,\n",
    "        'count': 2\n",
    "    },\n",
    "    {\n",
    "        'width': 128,\n",
    "        'output_width': 256,\n",
    "        'cardinality': 24,\n",
    "        'count': 3\n",
    "    },\n",
    "    {\n",
    "        'width': 256,\n",
    "        'output_width': 512,\n",
    "        'cardinality': 24,\n",
    "        'count': 2\n",
    "    }\n",
    "]\n",
    "config.iChannels = 32\n",
    "config.epochs = 120\n",
    "config.max_lr = 0.0016\n",
    "config.min_lr = 0.0004\n",
    "config.n_cycles = 8\n",
    "config.batch_size = 70\n",
    "config.validation_split = 0.08\n",
    "config.steps_per_epoch = int(200840*(1-config.validation_split))//config.batch_size//4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ResNeXt Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_block(x, iChannels):\n",
    "    x = Conv2D(iChannels, (7, 7), strides=2, padding='same', use_bias=False, kernel_initializer='he_normal',\n",
    "               kernel_regularizer=l2(WEIGHT_DECAY))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnext_block(x, width, output_width, cardinality):\n",
    "    x = Conv2D(filters=width, padding='same', kernel_size=3)(x)\n",
    "    inp = x\n",
    "    inp = Conv2D(output_width, padding='same', kernel_size=1)(inp)\n",
    "    subblocks = []\n",
    "\n",
    "    for i in range(cardinality):\n",
    "        y = Conv2D(filters=width, kernel_size=1)(x)\n",
    "        # y = BatchNormalization()(y)#name=f'bn_1_{np.random.random()}')(y)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Conv2D(filters=width, kernel_size=3, padding='same')(y)\n",
    "        # y = BatchNormalization()(y)#name=f'bn_3_{np.random.random()}')(y)\n",
    "        y = Activation('relu')(y)\n",
    "        subblocks.append(y)\n",
    "\n",
    "    x = Concatenate()(subblocks)\n",
    "    x = Conv2D(output_width, kernel_size=1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Add()([x, inp])\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnext(blocks, iChannels, input_size=(64, 64, 1)):\n",
    "    x = Input(shape=input_size)\n",
    "    inp = x\n",
    "\n",
    "    x = init_block(x, iChannels)\n",
    "\n",
    "    for b in blocks:\n",
    "        for i in range(b['count']):\n",
    "            x = resnext_block(x, b['width'], b['output_width'], b['cardinality'])\n",
    "        x = MaxPool2D()(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = Dense(2048, activation=\"relu\")(x)\n",
    "    x = Dropout(rate=0.12)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "\n",
    "    head_root = Dense(168, activation='softmax', name='dense_a')(x)\n",
    "    head_vowel = Dense(11, activation='softmax', name='dense_b')(x)\n",
    "    head_consonant = Dense(7, activation='softmax', name='dense_c')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=[head_root, head_vowel, head_consonant])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_resnext(config.blocks, config.iChannels)\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              loss_weights=[2, 1, 1], \n",
    "              metrics=['accuracy', keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_steps = int(200840*config.validation_split)//config.batch_size//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n",
    "\n",
    "    def flow(self,\n",
    "             x,\n",
    "             y=None,\n",
    "             batch_size=32,\n",
    "             shuffle=True,\n",
    "             sample_weight=None,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png',\n",
    "             subset=None):\n",
    "\n",
    "        targets = None\n",
    "        target_lengths = {}\n",
    "        ordered_outputs = []\n",
    "        for output, target in y.items():\n",
    "            if targets is None:\n",
    "                targets = target\n",
    "            else:\n",
    "                targets = np.concatenate((targets, target), axis=1)\n",
    "            target_lengths[output] = target.shape[1]\n",
    "            ordered_outputs.append(output)\n",
    "\n",
    "        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n",
    "                                         shuffle=shuffle):\n",
    "            target_dict = {}\n",
    "            i = 0\n",
    "            for output in ordered_outputs:\n",
    "                target_length = target_lengths[output]\n",
    "                target_dict[output] = flowy[:, i: i + target_length]\n",
    "                i += target_length\n",
    "\n",
    "            yield flowx, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction_root = ReduceLROnPlateau(monitor='dense_a_accuracy',\n",
    "                                                 patience=3,\n",
    "                                                 verbose=1,\n",
    "                                                 factor=0.5,\n",
    "                                                 min_lr=0.00001)\n",
    "learning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_b_accuracy',\n",
    "                                                  patience=3,\n",
    "                                                  verbose=1,\n",
    "                                                  factor=0.5,\n",
    "                                                  min_lr=0.00001)\n",
    "learning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_c_accuracy',\n",
    "                                                      patience=3,\n",
    "                                                      verbose=1,\n",
    "                                                      factor=0.5,\n",
    "                                                      min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = MultiOutputDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range=0.15,  # Randomly zoom image\n",
    "            width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False,   # randomly flip images\n",
    "            rescale=1.0/255.0,\n",
    "            validation_split=config.validation_split)\n",
    "\n",
    "train_df_['image_id'] = train_df_['image_id'].astype(str)+'.png'\n",
    "\n",
    "Y_train_root = pd.get_dummies(train_df_.set_index('image_id')['grapheme_root'], dtype=np.float32)\n",
    "Y_train_vowel = pd.get_dummies(train_df_.set_index('image_id')['vowel_diacritic'], dtype=np.float32)\n",
    "Y_train_consonant = pd.get_dummies(train_df_.set_index('image_id')['consonant_diacritic'], dtype=np.float32)\n",
    "\n",
    "def generator_wrapper(gen: MultiOutputDataGenerator, df: pd.DataFrame, subset: str, batch_size: int):\n",
    "    for flowx, flowy in gen.flow_from_dataframe(df, \n",
    "                                                color_mode='grayscale', \n",
    "                                                directory='data', \n",
    "                                                x_col='image_id',\n",
    "                                                y_col='image_id', \n",
    "                                                class_mode='raw', \n",
    "                                                target_size=(64, 64), \n",
    "                                                subset=subset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=True):\n",
    "        yield flowx, {\n",
    "            'dense_a': Y_train_root.loc[flowy].values,\n",
    "            'dense_b': Y_train_vowel.loc[flowy].values,\n",
    "            'dense_c': Y_train_consonant.loc[flowy].values,\n",
    "        }\n",
    "\n",
    "validation_generator = generator_wrapper(datagen, train_df_, 'validation', config.batch_size)\n",
    "model.fit_generator(generator_wrapper(datagen, train_df_, 'training', config.batch_size), \n",
    "                    validation_data=validation_generator, \n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=config.epochs, \n",
    "                    steps_per_epoch=config.steps_per_epoch,\n",
    "                    callbacks=[learning_rate_reduction_root, \n",
    "                               learning_rate_reduction_vowel, \n",
    "                               learning_rate_reduction_consonant,\n",
    "                               WandbCallback(data_type='image')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(wandb.run.dir, model_filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
