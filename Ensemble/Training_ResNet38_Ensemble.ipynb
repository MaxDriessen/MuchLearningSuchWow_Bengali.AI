{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali.AI Competition - ResNet-38 Training (Ensemble)\n",
    "\n",
    "### Team MuchLearningSuchWow\n",
    "\n",
    "This notebook contains code for training the ResNet-38 network we used in our ensemble. It is connected to Weights and Biases in order to keep track of progress and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import psutil\n",
    "\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, Add, MaxPool2D, Dense, \\\n",
    "    Dropout, GlobalAveragePooling2D, Concatenate, Input, Flatten, AveragePooling2D, Add\n",
    "from keras import Model\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import gc\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'input/bengaliai-cv19/train.csv'\n",
    "model_filename = 'output/model_resnet38.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('output'):\n",
    "    os.mkdir('output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = pd.read_csv(train_filename)\n",
    "train_df_ = train_df_.drop(['grapheme'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project='bengali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = run.config\n",
    "config.blocks = [\n",
    "    {\n",
    "        'width': 64,\n",
    "        'output_width': 128,\n",
    "        'cardinality': 4,\n",
    "        'count': 2\n",
    "    },\n",
    "    {\n",
    "        'width': 128,\n",
    "        'output_width': 256,\n",
    "        'cardinality': 4,\n",
    "        'count': 2\n",
    "    },\n",
    "    {\n",
    "        'width': 256,\n",
    "        'output_width': 512,\n",
    "        'cardinality': 4,\n",
    "        'count': 2\n",
    "    },\n",
    "    {\n",
    "        'width': 512,\n",
    "        'output_width': 1024,\n",
    "        'cardinality': 4,\n",
    "        'count': 3\n",
    "    },\n",
    "    {\n",
    "        'width': 1024,\n",
    "        'output_width': 1024,\n",
    "        'cardinality': 4,\n",
    "        'count': 3\n",
    "    }\n",
    "]\n",
    "config.iChannels = 32\n",
    "config.epochs = 180\n",
    "config.max_lr = 0.0016\n",
    "config.min_lr = 0.0004\n",
    "config.n_cycles = 8\n",
    "config.dropout = 0.4\n",
    "config.batch_size = 240\n",
    "config.validation_split = 0.08\n",
    "config.resnet_depth = 38\n",
    "config.steps_per_epoch = int(200840*(1-config.validation_split))//config.batch_size//4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ResNet-38 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1,\n",
    "                 activation='relu', batch_normalization=True, conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(depth, input_shape=(64, 64, 1)):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = Add()([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "\n",
    "    y = Dense(2048, activation=\"relu\")(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    dense = Dense(1024, activation=\"relu\", kernel_initializer='he_normal')(y)\n",
    "\n",
    "    head_root = Dense(168, activation='softmax', kernel_initializer='he_normal', name='dense_a')(dense)\n",
    "    head_vowel = Dense(11, activation='softmax', kernel_initializer='he_normal', name='dense_b')(dense)\n",
    "    head_consonant = Dense(7, activation='softmax', kernel_initializer='he_normal', name='dense_c')(dense)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_resnet(config.resnet_depth)\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              loss_weights=[2, 1, 1], \n",
    "              metrics=['accuracy', keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_steps = int(200840*config.validation_split)//config.batch_size//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n",
    "\n",
    "    def flow(self,\n",
    "             x,\n",
    "             y=None,\n",
    "             batch_size=32,\n",
    "             shuffle=True,\n",
    "             sample_weight=None,\n",
    "             seed=None,\n",
    "             save_to_dir=None,\n",
    "             save_prefix='',\n",
    "             save_format='png',\n",
    "             subset=None):\n",
    "\n",
    "        targets = None\n",
    "        target_lengths = {}\n",
    "        ordered_outputs = []\n",
    "        for output, target in y.items():\n",
    "            if targets is None:\n",
    "                targets = target\n",
    "            else:\n",
    "                targets = np.concatenate((targets, target), axis=1)\n",
    "            target_lengths[output] = target.shape[1]\n",
    "            ordered_outputs.append(output)\n",
    "\n",
    "        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n",
    "                                         shuffle=shuffle):\n",
    "            target_dict = {}\n",
    "            i = 0\n",
    "            for output in ordered_outputs:\n",
    "                target_length = target_lengths[output]\n",
    "                target_dict[output] = flowy[:, i: i + target_length]\n",
    "                i += target_length\n",
    "\n",
    "            yield flowx, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction_root = ReduceLROnPlateau(monitor='dense_a_accuracy',\n",
    "                                                 patience=4,\n",
    "                                                 verbose=1,\n",
    "                                                 factor=0.25,\n",
    "                                                 min_lr=1e-10)\n",
    "learning_rate_reduction_vowel = ReduceLROnPlateau(monitor='dense_b_accuracy',\n",
    "                                                  patience=4,\n",
    "                                                  verbose=1,\n",
    "                                                  factor=0.25,\n",
    "                                                  min_lr=1e-10)\n",
    "learning_rate_reduction_consonant = ReduceLROnPlateau(monitor='dense_c_accuracy',\n",
    "                                                      patience=4,\n",
    "                                                      verbose=1,\n",
    "                                                      factor=0.25,\n",
    "                                                      min_lr=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = MultiOutputDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range=0.15,  # Randomly zoom image\n",
    "            width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False,   # randomly flip images\n",
    "            rescale=1.0/255.0,\n",
    "            validation_split=config.validation_split)\n",
    "\n",
    "train_df_['image_id'] = train_df_['image_id'].astype(str)+'.png'\n",
    "\n",
    "Y_train_root = pd.get_dummies(train_df_.set_index('image_id')['grapheme_root'], dtype=np.float32)\n",
    "Y_train_vowel = pd.get_dummies(train_df_.set_index('image_id')['vowel_diacritic'], dtype=np.float32)\n",
    "Y_train_consonant = pd.get_dummies(train_df_.set_index('image_id')['consonant_diacritic'], dtype=np.float32)\n",
    "\n",
    "def generator_wrapper(gen: MultiOutputDataGenerator, df: pd.DataFrame, subset: str, batch_size: int):\n",
    "    for flowx, flowy in gen.flow_from_dataframe(df, \n",
    "                                                color_mode='grayscale', \n",
    "                                                directory='data', \n",
    "                                                x_col='image_id',\n",
    "                                                y_col='image_id', \n",
    "                                                class_mode='raw', \n",
    "                                                target_size=(64, 64), \n",
    "                                                subset=subset, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=True):\n",
    "        yield flowx, {\n",
    "            'dense_a': Y_train_root.loc[flowy].values,\n",
    "            'dense_b': Y_train_vowel.loc[flowy].values,\n",
    "            'dense_c': Y_train_consonant.loc[flowy].values,\n",
    "        }\n",
    "\n",
    "validation_generator = generator_wrapper(datagen, train_df_, 'validation', config.batch_size)\n",
    "model.fit_generator(generator_wrapper(datagen, train_df_, 'training', config.batch_size), \n",
    "                    validation_data=validation_generator, \n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=config.epochs, \n",
    "                    steps_per_epoch=config.steps_per_epoch,\n",
    "                    callbacks=[learning_rate_reduction_root, \n",
    "                               learning_rate_reduction_vowel, \n",
    "                               learning_rate_reduction_consonant,\n",
    "                               WandbCallback(data_type='image')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(wandb.run.dir, model_filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
