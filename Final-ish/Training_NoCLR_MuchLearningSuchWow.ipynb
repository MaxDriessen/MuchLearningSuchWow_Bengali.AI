{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali.AI Competition - Model Training\n",
    "\n",
    "### Team MuchLearningSuchWow\n",
    "\n",
    "This notebook contains the code that we used to train the convolutional network with class weights but no cyclical learning rate. The core of this code (network, training procedure, etc.) is based on [this kernel](https://www.kaggle.com/kaushal2896/bengali-graphemes-starter-eda-multi-output-cnn). This notebook uses the preprocessed data as produced by our preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import time, gc\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.models import clone_model\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization,Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, Callback, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=64\n",
    "N_CHANNELS=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\\bengaliai-cv19\\best_model_conv.hdf5\n",
      "input\\bengaliai-cv19\\class_map.csv\n",
      "input\\bengaliai-cv19\\sample_submission.csv\n",
      "input\\bengaliai-cv19\\test.csv\n",
      "input\\bengaliai-cv19\\test_image_data_0.parquet\n",
      "input\\bengaliai-cv19\\test_image_data_1.parquet\n",
      "input\\bengaliai-cv19\\test_image_data_2.parquet\n",
      "input\\bengaliai-cv19\\test_image_data_3.parquet\n",
      "input\\bengaliai-cv19\\train.csv\n",
      "input\\bengaliai-cv19\\train_image_data_0.parquet\n",
      "input\\bengaliai-cv19\\train_image_data_1.parquet\n",
      "input\\bengaliai-cv19\\train_image_data_2.parquet\n",
      "input\\bengaliai-cv19\\train_image_data_3.parquet\n",
      "input\\bengaliai-cv19\\train_image_data_preprocessed.parquet\n",
      "input\\bengaliai-cv19\\train_image_data_preprocessed_0.parquet\n",
      "input\\bengaliai-cv19\\train_image_data_preprocessed_1.parquet\n",
      "input\\bengaliai-cv19\\train_image_data_preprocessed_2.parquet\n",
      "input\\bengaliai-cv19\\train_image_data_preprocessed_3.parquet\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_filename = \"drive/My Drive/MLiP/input/bengaliai-cv19/train.csv\"\n",
    "train_filename = \"drive/My Drive/MLiP/input/bengaliai-cv19/train_image_data_preprocessed.parquet\"\n",
    "model_filename = \"drive/My Drive/MLiP/input/bengaliai-cv19/best_model_conv_NoCLR.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_df_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: (200840, 5)\n",
      "Size of test data: (36, 3)\n",
      "Size of class map: (186, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of training data: {train_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['grapheme'], axis=1, inplace=False)\n",
    "train_df[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_df[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(df):\n",
    "    cols = []\n",
    "    for col in df:\n",
    "        cols.append(pd.get_dummies(df[col].astype(str)))\n",
    "    return pd.concat(cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_convolutional():\n",
    "    inputs = Input(shape = (IMG_SIZE, IMG_SIZE, 1))\n",
    "\n",
    "    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1))(inputs)\n",
    "    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "    model = Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "    model = BatchNormalization(momentum=0.15)(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(1024, activation = \"relu\")(model)\n",
    "    model = Dropout(rate=0.3)(model)\n",
    "    dense = Dense(512, activation = \"relu\")(model)\n",
    "\n",
    "    head_root = Dense(168, activation = 'softmax', name='dense_root')(dense)\n",
    "    head_vowel = Dense(11, activation = 'softmax', name='dense_vowel')(dense)\n",
    "    head_consonant = Dense(7, activation = 'softmax', name='dense_consonant')(dense)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], loss_weights=[0.5, 0.25, 0.25])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 32)   320         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 32)   9248        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 32)   9248        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   25632       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 32)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 64)   18496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 64)   36928       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 64)   36928       conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 64)   36928       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   102464      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16, 16, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 128)  73856       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 128)  147584      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 128)  147584      conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 128)  147584      conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 128)  512         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 128)    409728      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 128)    512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 8, 8, 128)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 256)    295168      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 256)    590080      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 256)    590080      conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 256)    590080      conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 256)    1024        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 256)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 4, 4, 256)    1638656     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 4, 4, 256)    1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 4, 4, 256)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_root (Dense)              (None, 168)          86184       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_vowel (Dense)             (None, 11)           5643        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_consonant (Dense)         (None, 7)            3591        dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,735,098\n",
      "Trainable params: 9,733,242\n",
      "Non-trainable params: 1,856\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_convolutional()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grapheme = train_df[\"grapheme_root\"].to_numpy()\n",
    "train_vowel = train_df[\"vowel_diacritic\"].to_numpy()\n",
    "train_consonant = train_df[\"consonant_diacritic\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_grapheme = class_weight.compute_class_weight('balanced', np.unique(train_grapheme), train_grapheme)\n",
    "class_weights_vowel = class_weight.compute_class_weight('balanced', np.unique(train_vowel), train_vowel)\n",
    "class_weights_consonant = class_weight.compute_class_weight('balanced', np.unique(train_consonant), train_consonant)\n",
    "class_weights = [class_weights_grapheme, class_weights_vowel, class_weights_consonant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.13249109 8.24466338 3.54740709 3.75935909 3.61171055 6.83129252\n",
      " 3.8814162  7.81356987 7.61449803 2.69251394 7.86497494 7.91706086\n",
      " 8.18819309 0.22056756 1.50185451 1.10385613 1.27178318 1.57092798\n",
      " 0.7320736  4.30027407 3.55796485 1.269083   0.40374069 0.23217638\n",
      " 3.55796485 1.06075971 6.99108883 3.91959407 1.57506744 0.43002741\n",
      " 2.73564346 1.55660962 1.06075971 8.79026611 4.33143547 2.5115046\n",
      " 1.16745722 4.1946533  0.35643297 1.93756271 1.57922879 3.91959407\n",
      " 0.61087184 0.32933228 1.13100869 8.30191799 2.06116585 2.64485883\n",
      " 0.86880537 3.72422489 1.61988644 3.66710488 1.27858416 0.32397729\n",
      " 2.01938546 0.71159297 0.44474561 1.88858798 0.93033167 0.51110568\n",
      " 2.80628214 2.07908903 1.37727672 8.02333014 0.21363048 0.87580673\n",
      " 1.52096207 2.51679198 1.89457399 1.57922879 1.24919142 0.40717854\n",
      " 0.20841635 9.1959707  0.78753372 1.06075971 0.61749803 1.24919142\n",
      " 4.08012352 0.34571318 2.62165831 0.34772431 4.09409654 0.84307207\n",
      " 2.59886128 0.87709185 0.57090554 7.11592971 1.57299499 0.51685093\n",
      " 1.90666059 0.77678765 1.07121522 1.92198744 1.22864973 1.64439641\n",
      " 0.24268701 2.48539749 1.90666059 2.61021002 2.66847364 1.10385613\n",
      " 8.47855454 0.34541352 7.47172619 7.91706086 1.59184579 0.22467134\n",
      " 7.566305   1.31660373 3.49554442 1.51902947 1.34929593 0.27200823\n",
      " 7.96984127 0.29775248 2.74191787 0.78084663 1.04958401 0.77779843\n",
      " 0.99045252 2.5876108  0.51685093 0.57668895 0.54637851 1.47045042\n",
      " 7.51871818 1.28961833 1.2557523  1.22236829 8.30191799 2.69859185\n",
      " 1.15060269 0.27331417 1.91890239 1.87672871 1.13746545 2.12718183\n",
      " 1.27995309 0.51707448 1.35388017 0.68469427 1.1204088  1.96301509\n",
      " 1.95339247 3.77121827 3.95853043 0.27219403 0.69383412 0.49770033\n",
      " 0.51729822 0.95791361 1.96948302 0.76978506 1.63316419 1.28822865\n",
      " 1.513261   3.68974133 8.35997336 0.36436336 2.4905754  3.84397489\n",
      " 2.57091654 7.24531025 7.28948897 1.04682679 3.89405925 0.75424365]\n",
      "[0.43987139 0.49498948 0.7031302  1.13039759 0.96870659 3.44689104\n",
      " 4.21083529 0.63566417 5.1752216  1.13885865 5.12438446]\n",
      "[ 0.22902208  3.86468596  1.22273295 46.35125779  1.34891531  1.34090894\n",
      " 20.68596148]\n"
     ]
    }
   ],
   "source": [
    "print(class_weights_grapheme)\n",
    "print(class_weights_vowel)\n",
    "print(class_weights_consonant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n",
    "\n",
    "    def flow(self, x, y=None, batch_size=32, shuffle=True, sample_weight=None,  \n",
    "             seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None):\n",
    "\n",
    "        targets = None\n",
    "        target_lengths = {}\n",
    "        ordered_outputs = []\n",
    "        for output, target in y.items():\n",
    "            if targets is None:\n",
    "                targets = target\n",
    "            else:\n",
    "                targets = np.concatenate((targets, target), axis=1)\n",
    "            target_lengths[output] = target.shape[1]\n",
    "            ordered_outputs.append(output)\n",
    "\n",
    "\n",
    "        for flowx, flowy in super().flow(x, targets, batch_size=batch_size, shuffle=shuffle):\n",
    "            target_dict = {}\n",
    "            i = 0\n",
    "            for output in ordered_outputs:\n",
    "                target_length = target_lengths[output]\n",
    "                target_dict[output] = flowy[:, i: i + target_length]\n",
    "                i += target_length\n",
    "\n",
    "            yield flowx, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: (200840, 64, 64, 1)\n",
      "Training labels root: (200840, 168)\n",
      "Training labels vowel: (200840, 11)\n",
      "Training labels consonants: (200840, 7)\n",
      "Epoch 1/100\n",
      "  1/721 [..............................] - ETA: 2:41:10 - loss: 4.4020 - dense_root_loss: 5.9228 - dense_vowel_loss: 3.3037 - dense_consonant_loss: 2.4586 - dense_root_accuracy: 0.0039 - dense_vowel_accuracy: 0.1094 - dense_consonant_accuracy: 0.1367"
     ]
    }
   ],
   "source": [
    "train_df_full = pd.merge(pd.read_parquet(train_filename), train_df, on='image_id').drop(['image_id'], axis=1)\n",
    "\n",
    "X_train = train_df_full.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n",
    "    \n",
    "# CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n",
    "X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
    "    \n",
    "Y_train_root = pd.get_dummies(train_df_full['grapheme_root']).values\n",
    "Y_train_vowel = pd.get_dummies(train_df_full['vowel_diacritic']).values\n",
    "Y_train_consonant = pd.get_dummies(train_df_full['consonant_diacritic']).values\n",
    "\n",
    "print(f'Training images: {X_train.shape}')\n",
    "print(f'Training labels root: {Y_train_root.shape}')\n",
    "print(f'Training labels vowel: {Y_train_vowel.shape}')\n",
    "print(f'Training labels consonants: {Y_train_consonant.shape}')\n",
    "\n",
    "# Divide the data into training and validation set\n",
    "x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=666)\n",
    "del train_df_full\n",
    "del X_train\n",
    "del Y_train_root, Y_train_vowel, Y_train_consonant\n",
    "\n",
    "# Data augmentation for creating more training data\n",
    "datagen = MultiOutputDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.15, # Randomly zoom image \n",
    "    width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# This will just calculate parameters required to augment the given data. This won't perform any augmentations\n",
    "datagen.fit(x_train)\n",
    "    \n",
    "# Add saving only the best model\n",
    "mcp_save = ModelCheckpoint(model_filename, save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit_generator(datagen.flow(x_train, \n",
    "                                           {'dense_root': y_train_root, 'dense_vowel': y_train_vowel, 'dense_consonant': y_train_consonant}, \n",
    "                                           batch_size=batch_size), \n",
    "                              epochs=epochs, \n",
    "                              validation_data=(x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n",
    "                              steps_per_epoch=x_train.shape[0] // batch_size, \n",
    "                              callbacks=[mcp_save], class_weight=class_weights)\n",
    "\n",
    "# Delete to reduce memory usage\n",
    "del x_train\n",
    "del x_test\n",
    "del y_train_root\n",
    "del y_test_root\n",
    "del y_train_vowel\n",
    "del y_test_vowel\n",
    "del y_train_consonant\n",
    "del y_test_consonant\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Losses and Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def plot_loss(his, epoch, title):\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epoch), his.history['loss'], label='train_loss')\n",
    "    plt.plot(np.arange(0, epoch), his.history['dense_3_loss'], label='train_root_loss')\n",
    "    plt.plot(np.arange(0, epoch), his.history['dense_4_loss'], label='train_vowel_loss')\n",
    "    plt.plot(np.arange(0, epoch), his.history['dense_5_loss'], label='train_consonant_loss')\n",
    "    \n",
    "    plt.plot(np.arange(0, epoch), his.history['val_dense_3_loss'], label='val_train_root_loss')\n",
    "    plt.plot(np.arange(0, epoch), his.history['val_dense_4_loss'], label='val_train_vowel_loss')\n",
    "    plt.plot(np.arange(0, epoch), his.history['val_dense_5_loss'], label='val_train_consonant_loss')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_acc(his, epoch, title):\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epoch), his.history['dense_3_accuracy'], label='train_root_acc')\n",
    "    plt.plot(np.arange(0, epoch), his.history['dense_4_accuracy'], label='train_vowel_accuracy')\n",
    "    plt.plot(np.arange(0, epoch), his.history['dense_5_accuracy'], label='train_consonant_accuracy')\n",
    "    \n",
    "    plt.plot(np.arange(0, epoch), his.history['val_dense_3_accuracy'], label='val_root_acc')\n",
    "    plt.plot(np.arange(0, epoch), his.history['val_dense_4_accuracy'], label='val_vowel_accuracy')\n",
    "    plt.plot(np.arange(0, epoch), his.history['val_dense_5_accuracy'], label='val_consonant_accuracy')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history, epochs, 'Losses')\n",
    "plot_acc(history, epochs, 'Accuracies')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
